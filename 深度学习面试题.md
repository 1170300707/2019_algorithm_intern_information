## CNN权值共享问题
首先权值共享就是滤波器共享，滤波器的参数是固定的，即是用相同的滤波器去扫一遍图像，提取一次特征特征，得到feature map。在卷积网络中，学好了一个滤波器，就相当于掌握了一种特征，这个滤波器在图像中滑动，进行特征提取，然后所有进行这样操作的区域都会被采集到这种特征，就好比上面的水平线。

## CNN结构特点
局部连接，权值共享，池化操作，多层次结构。

局部连接使网络可以提取数据的局部特征；权值共享大大降低了网络的训练难度，一个Filter只提取一个特征，在整个图片（或者语音／文本） 中进行卷积；池化操作与多层次结构一起，实现了数据的降维，将低层次的局部特征组合成为较高层次的特征，从而对整个图片进行表示。如下图所示：

## 什么样的数据集不适合深度学习
+ 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势。
+ 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。
## 什么造成梯度消失问题
+ 神经网络的训练中，通过改变神经元的权重，使网络的输出值尽可能逼近标签以降低误差值，训练普遍使用BP算法，核心思想是，计算出输出与标签间的损失函数值，然后计算其相对于每个神经元的梯度，进行权值的迭代。
+ 梯度消失会造成权值更新缓慢，模型训练难度增加。造成梯度消失的一个原因是，许多激活函数将输出值挤压在很小的区间内，在激活函数两端较大范围的定义域内梯度为0，造成学习停止。
## Overfitting怎么解决
首先所谓过拟合，指的是一个模型过于复杂之后，它可以很好地“记忆”每一个训练数据中随机噪音的部分而忘记了去“训练”数据中的通用趋势。
过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。

Parameter Norm Penalties(参数范数惩罚)；Dataset Augmentation (数据集增强)；Early Stopping(提前终止)；Parameter Tying and Parameter Sharing (参数绑定与参数共享)；Bagging and Other Ensemble Methods(Bagging 和其他集成方法)；dropout；regularization； batch normalizatin。是解决Overfitting的常用手段。

## L1和L2区别
L1 范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 比如 向量 A=[1，-1，3]， 那么 A 的 L1 范数为 |1|+|-1|+|3|。简单总结一下就是： 

+ L1 范数: 为 x 向量各个元素绝对值之和。 
+ L2 范数: 为 x 向量各个元素平方和的 1/2 次方，L2 范数又称 Euclidean 范数或 Frobenius 范数 
+ Lp 范数: 为 x 向量各个元素绝对值 p 次方和的 1/p 次方.
在支持向量机学习过程中，L1 范数实际是一种对于成本函数求解最优的过程，因此，L1 范数正则化通过向成本函数中添加 L1 范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 

L1 范数可以使权值稀疏，方便特征提取。 L2 范数可以防止过拟合，提升模型的泛化能力。

## TensorFlow计算图
Tensorflow 是一个通过计算图的形式来表述计算的编程系统，计算图也叫数据流图，可以把计算图看做是一种有向图，Tensorflow 中的每一个计算都是计算图上的一个节点，而节点之间的边描述了计算之间的依赖关系。

## BN（批归一化）的作用
(1). 可以使用更高的学习率。如果每层的scale不一致，实际上每层需要的学习率是不一样的，同一层不同维度的scale往往也需要不同大小的学习率，通常需要使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization将每层、每维的scale保持一致，那么我们就可以直接使用较高的学习率进行优化。

(2). 移除或使用较低的dropout。 dropout是常用的防止overfitting的方法，而导致overfit的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfit现象就可以得到一定的缓解。论文中最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%-50%相比，可以大大提高训练速度。

(3). 降低L2权重衰减系数。 还是一样的问题，边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，现在用了Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。

(4). 取消Local Response Normalization层。 由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。

(5). Batch Normalization调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为0方差为1的分布，这保证了梯度的有效性，可以解决反向传播过程中的梯度问题。目前大部分资料都这样解释，比如BN的原始论文认为的缓解了Internal Covariate Shift(ICS)问题。

## 什么是梯度消失和爆炸，怎么解决？
当训练较多层数的模型时，一般会出现梯度消失问题（gradient vanishing problem）和梯度爆炸问题（gradient exploding problem）。注意在反向传播中，当网络模型层数较多时，梯度消失和梯度爆炸是不可避免的。
深度神经网络中的梯度不稳定性，根本原因在于前面层上的梯度是来自于后面层上梯度的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。
前面的层比后面的层梯度变化更小，故变化更慢，故引起了梯度消失问题。前面层比后面层梯度变化更快，故引起梯度爆炸问题。

解决梯度消失和梯度爆炸问题，常用的有以下几个方案：

+ 预训练模型 + 微调
+ 梯度剪切 + 正则化
+ relu、leakrelu、elu等激活函数
+ BN批归一化
+ CNN中的残差结构
+ LSTM结构
## RNN循环神经网络理解
循环神经网络（recurrent neural network, RNN）, 主要应用在语音识别、语言模型、机器翻译以及时序分析等问题上。
***在经典应用中，卷积神经网络在不同的空间位置共享参数，循环神经网络是在不同的时间位置共享参数，从而能够使用有限的参数处理任意长度的序列。***
RNN可以看做作是同一神经网络结构在时间序列上被复制多次的结果，这个被复制多次的结构称为循环体，如何设计循环体的网络结构是RNN解决实际问题的关键。
RNN的输入有两个部分，一部分为上一时刻的状态，另一部分为当前时刻的输入样本。

## 训练过程中模型不收敛，是否说明这个模型无效，致模型不收敛的原因有哪些?
不一定。导致模型不收敛的原因有很多种可能，常见的有以下几种：

+ 没有对数据做归一化。
+ 没有检查过你的结果。这里的结果包括预处理结果和最终的训练测试结果。
+ 忘了做数据预处理。
+ 忘了使用正则化。
+ Batch Size设的太大。
+ 学习率设的不对。
+ 最后一层的激活函数用的不对。
+ 网络存在坏梯度。比如Relu对负值的梯度为0，反向传播时，0梯度就是不传播。
+ 参数初始化错误。
+ 网络太深。隐藏层神经元数量错误。
+ 更多回答，参考此链接。

## 图像处理中平滑和锐化操作是什么？
平滑处理（smoothing）也称模糊处理（bluring），主要用于消除图像中的噪声部分，平滑处理常用的用途是用来减少图像上的噪点或失真，平滑主要使用图像滤波。在这里，我个人认为可以把图像平滑和图像滤波联系起来，因为图像平滑常用的方法就是图像滤波器。
在OpenCV3中常用的图像滤波器有以下几种：

+ 方框滤波——BoxBlur函数
+ 均值滤波（邻域平均滤波）——Blur函数
+ 高斯滤波——GaussianBlur函数
+ 中值滤波——medianBlur函数
+ 双边滤波——bilateralFilter函数
图像锐化操作是为了突出显示图像的边界和其他细节，而图像锐化实现的方法是通过各种算子和滤波器实现的——Canny算子、Sobel算子、Laplacian算子以及Scharr滤波器。

## VGG使用2个3*3卷积的优势在哪里？
(1). 减少网络层参数。用两个3*3卷积比用1个5*5卷积拥有更少的参数量，只有后者的2∗3∗35∗5=0.72。但是起到的效果是一样的，两个3*3的卷积层串联相当于一个5*5的卷积层，感受野的大小都是5×5，即1个像素会跟周围5*5的像素产生关联。把下图当成动态图看，很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野。

！[2个３*卷积层](https://www.armcvai.com/wp-content/uploads/2019/03/2个33卷积层.png)
(2). 更多的非线性变换。2个3*3卷积层拥有比1个5*5卷积层更多的非线性变换（前者可以使用两次ReLU激活函数，而后者只有一次），使得卷积神经网络对特征的学习能力更强。

***paper中给出的相关解释***：三个这样的层具有7×7的有效感受野。那么我们获得了什么？例如通过使用三个3×3卷积层的堆叠来替换单个7×7层。首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。其次，我们减少参数的数量：假设三层3×3卷积堆叠的输入和输出有C个通道，堆叠卷积层的参数为3(32C2)=27C2个权重；同时，单个7×7卷积层将需要72C2=49C2个参数，即参数多81％。这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。

此回答可以参考TensorFlow实战p110，网上很多回答都说的不全。

## Relu比Sigmoid效果好在哪里？
ReLU激活函数公式如下：

![relu激活函数](http://latex.codecogs.com/gif.latex?%24%24ReLu%28x%29%3D%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20x%20%26%20x%3E0%5C%5C%200%20%26%20x%3C%3D0%20%5Cend%7Bmatrix%7D%5Cright.%24%24)
relu函数方程
ReLU 的输出要么是 0, 要么是输入本身。虽然方程简单，但实际上效果更好。在网上看了很多版本的解释，有从程序实例分析也有从数学上分析，我找了个相对比较直白的回答，如下：
(1). ReLU函数计算简单，可以减少很多计算量。反向传播求误差梯度时，涉及除法，计算量相对较大，采用ReLU激活函数，可以节省很多计算量；
(2). 避免梯度消失问题。对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失问题（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。
(3). 可以缓解过拟合问题的发生。Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。

#### 参考链接
[ReLU为什么比Sigmoid效果好](https://www.twblogs.net/a/5c2dd30fbd9eee35b21c4337/zh-cn)

## 神经网络中权值共享的理解？
权值(权重)共享这个词是由LeNet5模型提出来的。以CNN为例，在对一张图偏进行卷积的过程中，使用的是同一个卷积核的参数。
比如一个3×3×1的卷积核，这个卷积核内9个的参数被整张图共享，而不会因为图像内位置的不同而改变卷积核内的权系数。说的再直白一些，就是用一个卷积核不改变其内权系数的情况下卷积处理整张图片（当然CNN中每一层不会只有一个卷积核的，这样说只是为了方便解释而已）。

#### 参考资料
[如何理解CNN中的权值共享](https://blog.csdn.net/chaipp0607/article/details/73650759)

## 对fine-tuning(微调模型的理解)，为什么要修改最后几层神经网络权值？
使用预训练模型的好处，在于利用训练好的SOTA模型权重去做特征提取，可以节省我们训练模型和调参的时间。

至于为什么只微调最后几层神经网络权重，是因为：
(1). CNN中更靠近底部的层（定义模型时先添加到模型中的层）编码的是更加通用的可复用特征，而更靠近顶部的层（最后添加到模型中的层）编码的是更专业业化的特征。微调这些更专业化的特征更加有用，它更代表了新数据集上的有用特征。
(2). 训练的参数越多，过拟合的风险越大。很多SOTA模型拥有超过千万的参数，在一个不大的数据集上训练这么多参数是有过拟合风险的，除非你的数据集像Imagenet那样大。

#### 参考资料
Python深度学习p127.

## 什么是dropout?
dropout可以防止过拟合，dropout简单来说就是：我们在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型的泛化性更强，因为它不会依赖某些局部的特征。

![dropou直观展示](https://www.armcvai.com/wp-content/uploads/2019/03/dropout.png)
#### dropout具体工作流程
以 标准神经网络为例，正常的流程是：我们首先把输入数据x通过网络前向传播，然后把误差反向传播一决定如何更新参数让网络进行学习。使用dropout之后，过程变成如下：

(1). 首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变（图3中虚线为部分临时被删除的神经元）；
(2). 然后把输入x通过修改后的网络进行前向传播计算，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数（w，b）；
(3). 然后重复这一过程：
+ 恢复被删掉的神经元（此时被删除的神经元保持原样没有更新w参数，而没有被删除的神经元已经有所更新）
+ 从隐藏层神经元中随机选择一个一半大小的子集临时删除掉（同时备份被删除神经元的参数）。
+ 对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。
#### dropout在神经网络中的应用
(1). 在训练模型阶段

不可避免的，在训练网络中的每个单元都要添加一道概率流程，标准网络和带有dropout网络的比较图如下所示：

![dropout在训练阶段](https://www.armcvai.com/wp-content/uploads/2019/03/添加了dropou的神经网络图.png)
(2). 在测试模型阶段

预测模型的时候，输入是当前输入，每个神经单元的权重参数要乘以概率p。

![dropout在测试模型时](https://www.armcvai.com/wp-content/uploads/2019/03/dropout_test.png)
#### 如何选择dropout 的概率
input 的dropout概率推荐是0.8， hidden layer 推荐是0.5， 但是也可以在一定的区间上取值。（All dropout nets use p = 0.5 for hidden units and p = 0.8 for input units.）

#### 参考资料
1. [Dropout:A Simple Way to Prevent Neural Networks from Overfitting]
2. [深度学习中Dropout原理解析](https://zhuanlan.zhihu.com/p/38200980)

## HOG算法原理描述
方向梯度直方图（Histogram of Oriented Gradient, HOG）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。在深度学习取得成功之前，Hog特征结合SVM分类器被广泛应用于图像识别中，在行人检测中获得了较大的成功。

#### HOG特征原理
HOG的核心思想是所检测的局部物体外形能够被光强梯度或边缘方向的分布所描述。通过将整幅图像分割成小的连接区域（称为cells），每个cell生成一个方向梯度直方图或者cell中pixel的边缘方向，这些直方图的组合可表示出（所检测目标的目标）描述子。为改善准确率，局部直方图可以通过计算图像中一个较大区域(称为block)的光强作为measure被对比标准化，然后用这个值(measure)归一化这个block中的所有cells。这个归一化过程完成了更好的照射/阴影不变性。
与其他描述子相比，HOG得到的描述子保持了几何和光学转化不变性（除非物体方向改变）。因此HOG描述子尤其适合人的检测。

HOG特征提取方法就是将一个image：
1. 灰度化（将图像看做一个x,y,z（灰度）的三维图像）
2. 划分成小cells（2*2）
3. 计算每个cell中每个pixel的gradient（即orientation）
4. 统计每个cell的梯度直方图（不同梯度的个数），即可形成每个cell的descriptor。
#### HOG特征检测步骤
![HOG特征检测步骤](https://www.armcvai.com/wp-content/uploads/2019/03/image.png)
颜色空间归一化——–>梯度计算————->梯度方向直方图———->重叠块直方图归一化———–>HOG特征
#### 参考资料
[HOG特征检测－简述](https://blog.csdn.net/liyuqian199695/article/details/53835989)
## 移动端深度学习框架知道哪些，用过哪些？
知名的有TensorFlow Lite、小米MACE、腾讯的ncnn等，目前都没有用过。
## 如何提升网络的泛化能力
和防止模型过拟合的方法类似，另外还有模型融合方法。
## BN算法，为什么要在后面加加伽马和贝塔，不加可以吗？
最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入。不加也可以。
## 激活函数的作用
**激活函数实现去线性化**。神经元的结构的输出为所有输入的加权和，这导致神经网络是一个线性模型。如果将每一个神经元（也就是神经网络的节点）的输出通过一个非线性函数，那么整个神经网络的模型也就不再是线性的了，这个非线性函数就是激活函数。
常见的激活函数有：ReLU函数、sigmoid函数、tanh函数。
ReLU函数：$f(x)=max(x,0)$，
sigmoid函数：$f(x)=\frac{1}{1+e^{-x}}$
tanh函数：$f(x)=\frac{1+e^{-2x}}{1+e^{-2x}}$
## 卷积层和池化层有什么区别
(1). 卷积层有参数，池化层没有参数。
#### 卷积层参数数量计算方法
假设输入层矩阵维度是96*96*3，第一层卷积层使用尺寸为5*5、深度为16的过滤器（卷积核尺寸为5*5、卷积核数量为16），那么这层卷积层的参数个数为５*5*3*16+16=1216个
## Reference
1. [《Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift》阅读笔记与实现](https://blog.csdn.net/happynear/article/details/44238541)
2. [深度学习中 Batch Normalization为什么效果好](https://blog.csdn.net/happynear/article/details/44238541)
3. [详解机器学习中的梯度消失、爆炸原因及其解决方法](https://blog.csdn.net/qq_25737169/article/details/78847691)


