## 神经网络的压缩方法
我们知道，在一定程度上，网络越深，参数越多，模型越复杂，其最终效果越好。神经网络的压缩算法是，旨在将一个庞大而复杂的预训练模型（pre-trained model）转化为一个精简的小模型。
按照压缩过程对网络结构的破坏程度，我们将模型压缩技术分为“前端压缩”和“后端压缩”两部分。
+ 前端压缩，是指在不改变原网络结构的压缩技术，主要包括`知识蒸馏`、紧凑的模型结构涉及以及`滤波器（filter）层面的剪枝`等；
+ 后端压缩，是指包括`低秩近似`、未加限制的剪枝、`参数量化`以及二值网络等，目标在于尽可能减少模型大小，会对原始网络结构造成极大程度的改造。

总结：前端压缩几乎不改变原有网络结构（仅仅只是在原模型基础上减少了网络的层数或者滤波器个数），后端压缩对网络结构有不可逆的大幅度改变，造成原有深度学习库、甚至硬件设备不兼容改变之后的网络。其维护成本很高。
## 低秩近似
简单理解就是，卷积神经网络的权重矩阵往往稠密且巨大，从而计算开销大，有一种办法是**采用低秩近似的技术**将该稠密矩阵由若干个小规模矩阵近似重构出来，这种方法归类为低秩近似算法。
> 一般地，行阶梯型矩阵的秩等于其“台阶数”-非零行的行数。

**低秩近似算法能减小计算开销的原理**如下：
给定权重矩阵$W\epsilon R^{m*n}$，若能将其表示为若干个低秩矩阵的组合，即$W=\sum_{i=1}^{n}\alpha _{i}M_{i}$，其中$W\epsilon R^{m*n}$为低秩矩阵，其秩为$r_{i}$，并满足$r_{i}<<min(m,n)$，则其每一个低秩矩阵都可分解为小规模矩阵的乘积，$M_{i}=G_{i}H_{i}{T}$，其中$G_{i}\epsilon R^{m*r_{i}}$，$H_{i}\epsilon R^{m*r_{i}}$。当$r_{i}$取值很小时，便能大幅降低总体的存储和计算开销。
基于以上想法，Sindhwani等人提出使用结构化矩阵来进行低秩分解的算法，具体原理可自行参考论文。另一种比较简便的方法是使用矩阵分解来降低权重矩阵的参数，如Denton等人提出使用奇异值分解（Singular Value Decomposition，简称SVD）分解来重构全连接层的权重。
### 总结
低秩近似算法在中小型网络模型上，取得了很不错的效果，但其超参数量与网络层数呈线性变化趋势，随着网络层数的增加与模型复杂度的提升，其搜索空间会急剧增大。
## 剪枝与稀疏约束
给定一个预训练好的网络模型，常用的剪枝算法一般都遵从如下操作：
1. 衡量神经元的重要程度
2. 移除掉一部分不重要的神经元，这步比前 1 步更加简便，灵活性更高
3. 对网络进行微调，剪枝操作不可避免地影响网络的精度，为防止对分类性能造成过大的破坏，需要对剪枝后的模型进行微调。对于大规模行图像数据集（如ImageNet）而言，微调会占用大量的计算资源，因此对网络微调到什么程度，是需要斟酌的
4. 返回第一步，循环进行下一轮剪枝

基于以上循环剪枝框架，不同学者提出了不同的方法，Han等人提出首先**将低于某个阈值的权重连接全部剪除**，之后对剪枝后的网络进行微调以完成参数更新的方法，这种方法的不足之处在于，剪枝后的网络是非结构化的，即被剪除的网络连接在分布上，没有任何连续性，这种稀疏的结构，导致CPU高速缓冲与内存频繁切换，从而限制了实际的加速效果。
基于此方法，有学者尝试将剪枝的粒度提升到整个滤波器级别，即丢弃整个滤波器，但是如何衡量滤波器的重要程度是一个问题，其中一种策略是基于滤波器权重本身的统计量，如分别计算每个滤波器的 L1 或 L2 值，将相应数值大小作为衡量重要程度标准。
利用稀疏约束来对网络进行剪枝也是一个研究方向，其思路是在网络的优化目标中加入权重的稀疏正则项，使得训练时网络的部分权重趋向于 0 ，而这些 0 值就是剪枝的对象。
### 总结
总体而言，剪枝是一项有效减小模型复杂度的通用压缩技术，其关键之处在于`如何衡量个别权重对于整体模型的重要程度`。剪枝操作对网络结构的破坏程度极小，将剪枝与其他后端压缩技术相结合，能够达到网络模型最大程度压缩。
## 参数量化
相比于剪枝操作，参数量化则是一种常用的后端压缩技术。所谓“量化”，是指从权重中归纳出若干“代表”，由这些“代表”来表示某一类权重的具体数值。“代表”被存储在码本（codebook）之中，而原权重矩阵只需记录各自“代表”的索引即可，从而**极大地降低了存储开销**。这种思想可类比于经典的词包模型（bag-of-words model）。常用量化算法如下：
1. 最基本的一种量化算法是标量量化（scalar quantization），该算法基本思路是，对于每一个权重矩阵$W\epsilon R^{1*mn}$，首先将其转化为向量形式：$w\epsilon R^{1*mn}$。之后对该权重向量的元素进行$k$个簇的聚类，这可借助于经典的k-均值（k-means）聚类算法快速完成：
$$\underset{c}{arg min}\sum_{i}^{mn}\sum_{j}^{k}\begin{Vmatrix}
w_{i}-c_{j}
\end{Vmatrix}_{2}^{2}$$
这样，只需将$k$个聚类中心（$c_{j}$,标量）存储在码本中，而原权重矩阵则只负责记录各自聚类中心在码本中索引。如果不考虑码本的存储开销，该算法能将存储空间减少为原来的$log_{2}(k)/32$。基于$k$均值算法的标量量化在很多应用中非常有效。
2. 标量量化会在一定程度上降低网络的精度，为避免这个弊端，很多算法考虑结构化的向量方法，其中一种是乘积向量（Product Quantization, PQ），详情咨询查阅论文。
3. 以PQ方法为基础，Wu等人设计了一种通用的网络量化算法：QCNN(quantized CNN)，主要思想在于Wu等人认为最小化每一层网络输出的重构误差，比最小化量化误差更有效。

这三类基于聚类的参数量化算法，其本质思想在于将多个权重映射到同一个数值，从而实现权重共享，降低存储开销的目的。
### 总结
参数量化是一种常用的后端压缩技术，能够以很小的性能损失实现模型体积的大幅下降，不足之处在于，量化的网络是“固定”的，很难对其做任何改变，同时这种方法通用性差，需要配套专门的深度学习库来运行网络。
## 知识蒸馏
知识蒸馏（knowledge distillation），是迁移学习（transfer learning）的一种，简单来说就是训练一个大模型和一个小模型，将庞大而复杂的大模型学习到的知识，通过一定技术手段迁移到精简的小模型上，从而使小模型能够获得与大模型相近的性能。
## 参考资料
魏秀参-CNN解析神经网络
